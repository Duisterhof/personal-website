<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bardienus Pieter Duisterhof</title>
  
  <meta name="author" content="Bardienus Duisterhof">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

  <!-- Global site tag (gtag.js) - Google Analytics DO NOT COPY THIS -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-75286303-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-75286303-1');
</script>

</head>
 
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bardienus Pieter Duisterhof</name>
              </p>
                <p style="text-align:center">
                  <strong>Email:</strong> bduister@cmu.edu
                </p>
             <strong>Bio </strong> </p>
             I am a Ph.D. student at Carnegie Mellon University (CMU), in the <a href="https://www.ri.cmu.edu/"> Robotics Institute</a> advised by <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>. 
             I am currently a research intern in the DUSt3R Group at NAVER Labs Europe, advised by <a href="https://europe.naverlabs.com/people_user_naverlabs/j%C3%A9rome-revaud/">JÃ©rome Revaud</a> and <a href="https://europe.naverlabs.com/people_user_naverlabs/vincent-leroy/">Vincent Leroy</a>.
             My interests lie at the intersection of perception and robot manipulation of challenging objects, such as (transparent) deformables. 
             I am a recipient of the 2023 <a href="https://www.cs.cmu.edu/cmlh/">CMLH Fellowship in Digital Health Innovation</a>. 
             
             
              </p>
             During my first year at CMU I worked with <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/"> Sebastian Scherer </a>, where we focused on geometric camera calibration.        
             Prior to that I completed a Bachelor's and Master's degree in Aerospace Engineering at Delft University of Technology, in the Netherlands. Advised by <a href="http://www.bene-guido.eu/wordpress/">Guido de Croon</a>, I studied efficient bio-inspired algorithms for fully autonomous nano drones.
             In 2019 I was a visiting student at <a href="https://edge.seas.harvard.edu/">Vijay Janapa Reddi's Edge Computing lab</a>, at Harvard University, where we studied Deep Reinforcement Learning for tiny robots.  
            </p>
             I am passionate about creating a future where complex robotic automation is scalable, safe, and beneficial. 
            </p>
            <strong>Students </strong> </p> I am always looking for collaborators, shoot me an email if you would like to work with me!

              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=LLsYMFYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/bduisterhof">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/duisterhof/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/duisterhof/">Github</a>
              
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <!--bullet points with news-->
              <ul>
                <li>
                  [September, 2024] <a href="https://kth-rpl.github.io/cloth-splatting/">Cloth-Splatting</a> has been accepted at <a href="https://www.corl.org/">CoRL 2024</a>!
                </p>
                </li>
                <li>
                  [August, 2024] <a href="https://deformgs.github.io/">DeformGS</a> has been accepted at <a href="https://www.algorithmic-robotics.org/">WAFR 2024</a>!
                </p>
                </li>
                <li>
                  [July, 2024] I started my internship at NAVER Labs Europe! Excited to work with JÃ©rome Revaud, Vincent Leroy and the rest of the DUSt3R team. 
                </p>
                </li>
                
                <li>                [January, 2024] 2 papers accepted at <a href="https://2024.ieee-icra.org/">ICRA 2024</a>! See you in Japan ðŸ‡¯ðŸ‡µ. </p>

                </li>
                <li>                [November, 2023] Check out our recent work on  <a href="https://md-splatting.github.io">MD-Splatting</a>, a method for dense tracking and novel view synthesis of cloth ðŸ§£. </p>
                </li>
                <li>                [July, 2023] Our paper on NeRFs for transparent objects has been accepted for a ðŸŒŸspotlightðŸŒŸ presentation at the  <a href="https://sites.google.com/view/iccv23trickyworkshop">ICCV23 - TRICKY Workshop</a>. </p>
                </li>
                <li>                [April, 2023] Thanks to the <a href="https://www.cs.cmu.edu/cmlh/">CMLH Fellowship in Digital Health Innovation</a> for generously supporting my research! </p>
                </li>
              </ul>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="mast3r_sfm_stop()" onmouseover="mast3r_sfm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deformgs-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mast3r_sfm.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
              <script type="text/javascript">
                function mast3r_sfm_start() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }

                function mast3r_sfm_stop() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }
                mast3r_sfm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://europe.naverlabs.com/research/publications/mast3r-sfm-a-fully-integrated-solution-for-unconstrained-structure-from-motion/">
              <papertitle>MASt3R-SfM: a fully-Integrated solution for unconstrained Structure-from-Motion
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://www.vicos.si/people/lojze_zust/">Lojze Zust</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/pweinzae/">Philippe Weinzaepfel</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/vleroy/">Vincent Leroy</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/ycabon/">Yohann Cabon</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/jrevaud">JÃ©rome Revaud</a>
              <br>
              <br>
              <a href="https://arxiv.org/abs/2409.19152">arXiv</a>
              /
              <a href="https://github.com/naver/mast3r/tree/mast3r_sfm">code</a>
              <p></p>
              <p>MASt3R for 1000+ unordered images!</p>
            </td>
          </tr>



          <tr onmouseout="deformgs_stop()" onmouseover="deformgs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deformgs-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/deformgs.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
              <script type="text/javascript">
                function deformgs_start() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }

                function deformgs_stop() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }
                deformgs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://deformgs.github.io">
              <papertitle>DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://mandizhao.github.io/">Zhao Mandi</a>,
              <a href="https://www.ri.cmu.edu/ri-people/yunchao-yao/">Yunchao Yao</a>,
              <a href="https://jia-wei-liu.github.io/">Jia-Wei Liu</a>,
              <a href="https://jennyseidenschwarz.github.io/">Jenny Seidenschwarz</a>,
              <a href="https://sites.google.com/view/showlab">Mike Zheng Shou</a>,
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
              <a href="https://shurans.github.io/">Shuran Song</a>,
              <a href="https://research.nvidia.com/person/stan-birchfield">Stan Birchfield</a>,
              <a href="https://wenbowen123.github.io/">Bowen Wen</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>
              <br>
							<em>WARF 2024</em> 
              <br>
              <a href="https://md-splatting.github.io/">project website</a>
              /
              <a href="https://arxiv.org/abs/2312.00583">arXiv</a>
              /
              <a href="https://drive.google.com/drive/folders/116XTLBUvuiEQPjKXKZP8fYab3F3L1cCd?usp=sharing">data</a>
              /
              <a href="https://github.com/momentum-robotics-lab/md-splatting">code</a>
              /
              <a href="https://x.com/BDuisterhof/status/1829170197829431695">X thread</a>
              <p></p>
              <p>Deformable objects are common in household, industrial and healthcare settings. Tracking them would unlock many applications in robotics, gen-AI, and AR.

                How? Check out DeformGS: a method for dense 3D tracking and dynamic novel view synthesis on deformable cloths in the real world.</p>
            </td>
          </tr>




          <tr onmouseout="clothsplatting_stop()" onmouseover="clothsplatting_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='clothsplatting-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/clothsplatting.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video></div>
              </div>
              
              
              <script type="text/javascript">
                function clothsplatting_start() {
                  document.getElementById('clothsplatting-vid').style.opacity = "1";
                }

                function clothsplatting_stop() {
                  document.getElementById('clothsplatting-vid').style.opacity = "1";
                }
                clothsplatting_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://kth-rpl.github.io/cloth-splatting/">
              <papertitle>Cloth-Splatting: 3D State Estimation from RGB Supervision for Deformable Objects
              </papertitle>
              </a>
              <br>
              <a href="https://albilo17.github.io/">Alberta Longhini*</a>,
              <a href="https://buesma.github.io/">Marcel BÃ¼sching*</a>,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://jenslundell.ai/">Jens Lundell</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="https://www.kth.se/profile/celle">MÃ¥rten BjÃ¶rkman</a>
              <a href="https://www.kth.se/profile/dani">Danica Kragic</a>

              <br>
							<em>CoRL 2024</em> 
              <br>
              <a href="https://kth-rpl.github.io/cloth-splatting/">project website</a>
              /
              <a href="https://openreview.net/forum?id=WmWbswjTsi">paper</a>
              <p></p>
              <p> We present Cloth-Splatting: a method for accurate state estimation of deformable objects from RGB supervision. 
                Cloth-Splatting leverages a GNN as a prior to improve tracking accuracy and speed up convergence.</p>
            </td>
          </tr>


          <tr onmouseout="dynomo_stop()" onmouseover="dynomo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dynomo-vid'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                </video></div>
                <img src='images/dynomo.png' width="160">
              </div>
              
              
              <script type="text/javascript">
                function dynomo_start() {
                  document.getElementById('dynomo-vid').style.opacity = "1";
                }

                function dynomo_stop() {
                  document.getElementById('dynomo-vid').style.opacity = "1";
                }
                dynomo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.02104">
              <papertitle>DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction
              </papertitle>
              </a>
              <br>
              <a href="https://jennyseidenschwarz.github.io/">Jenny Seidenschwarz</a>,
              <a href="https://dvl.in.tum.de/team/zhouq/">Qunjie Zhou</a>,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
              <a href="https://dvl.in.tum.de/team/lealtaixe/">Laura Leal-TaixÃ©</a>

              <br>
							<em>Under Review</em> 
              <br>
              <a href="https://arxiv.org/abs/2409.02104">arXiv</a>
              <p></p>
              <p> Online 3D tracking can unlock many new applications in robotics, AR and VR. Most prior works have focused on offline tracking, requiring an entire sequence of posed images. 
                Here we present DynOMo, a method for simultaneous 3D tracking, 3D reconstruction, novel view synthesis and pose estimation!</p>
            </td>
          </tr>

          <tr onmouseout="residualnerf_stop()" onmouseover="residualnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='residualnerf_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/residual_nerf.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function residualnerf_start() {
                  document.getElementById('residualnerf_image').style.opacity = "1";
                }

                function residualnerf_stop() {
                  document.getElementById('residualnerf_image').style.opacity = "1";
                }
                residualnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://residual-nerf.github.io/">
              <papertitle>Residual-NeRF: Learning Residual NeRFs for Transparent Object Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://yueminm.github.io/">Yuemin Mao</a>,
              <a href="https://www.ri.cmu.edu/ri-people/si-heng-teng/">Si Heng Teng</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>
              <br>
							<em>ICRA 2024</em> 
              <br>
              <a href="https://residual-nerf.github.io/">project website</a>
              /
              <a href="https://arxiv.org/abs/2405.06181">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=oA2CkRFI61c">video</a>
              /
              <a href="https://github.com/momentum-robotics-lab/residual-nerf">code</a>
              <p></p>
              <p>In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. 
                By first learning a background NeRF of the scene without transparent objects to be manipulated, we improve depth perception quality and speed up training. 
              </p>
            </td>
          </tr>

          <tr onmouseout="gslbench_stop()" onmouseover="gslbench_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gslbench_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/gslbench.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function gslbench_start() {
                  document.getElementById('gslbench_image').style.opacity = "1";
                }

                function gslbench_stop() {
                  document.getElementById('gslbench_image').style.opacity = "1";
                }
                gslbench_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/gslbench/home">
              <papertitle>GSL-Bench: High Fidelity Gas Source Localisation Benchmarking
              </papertitle>
              </a>
              <br>
              Hajo H. Erwich,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido de Croon</a>
              <br>
							<em>ICRA 2024</em> 
              <br>
              <a href="https://sites.google.com/view/gslbench/home">project website</a>
              /
              <a href="https://ieeexplore.ieee.org/document/10610755">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=kZa48WXf_1w">video</a>
              /
              <a href="https://github.com/Herwich012/GSL-Bench">code</a>
              <p></p>
              <p>Gas-source localization is an important task for autonomous robots. We present GSL-Bench, the first standardized benchmark for gas-source localization. 
              GSL-Bench uses NVIDIA Isaac Sim for high visual fidelity, and OpenFOAM for realistic gas simulations. 
              </p>
            </td>
          </tr>



          <tr onmouseout="tartancalib_stop()" onmouseover="tartancalib_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tartan_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/tartancalib.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function tartancalib_start() {
                  document.getElementById('tartan_image').style.opacity = "1";
                }

                function tartancalib_stop() {
                  document.getElementById('tartan_image').style.opacity = "1";
                }
                tartancalib_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://tartancalib.com">
              <papertitle>TartanCalib: Iterative Wide-Angle Lens Calibration using Adaptive SubPixel Refinement of AprilTags
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://www.huyaoyu.com/">Yaoyu Hu</a>,
              <a href="https://www.ri.cmu.edu/ri-people/si-heng-teng/">Si Heng Teng</a>,
              <a href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/">Sebastian Scherer</a>
              <a href="https://tartancalib.com">project website</a>
              /
              <a href="https://arxiv.org/abs/2210.02511">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=8WCCS-Ag_Co">video</a>
              /
              <a href="https://github.com/castacks/tartancalib">code</a>
              <p></p>
              <p>In this work we present our methodology for accurate wide-angle calibration. Our pipeline generates an intermediate model, and leverages it to iteratively improve feature detection and eventually the camera parameters. </p>
            </td>
          </tr>

          <tr onmouseout="sniffy_stop()" onmouseover="sniffy_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sniffy_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/sniffy_square.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/sniffy.png' width="160">
              </div>
              <script type="text/javascript">
                function sniffy_start() {
                  document.getElementById('sniffy_image').style.opacity = "1";
                }

                function sniffy_stop() {
                  document.getElementById('sniffy_image').style.opacity = "1";
                }
                sniffy_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2107.05490">
              <papertitle>Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments</papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://shushuai3.github.io/">Shushuai Li</a>,
              <a href="https://www.javierburgues.com/">Javier BurguÃ©s</a>,
              <a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janapa Reddi</a>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido C.H.E. de Croon</a>
              <br>
							<em>IROS</em>, 2021 
              <br>
      
              <a href="https://arxiv.org/abs/2107.05490">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=hj_SBSpK5qg">video</a>
              /
              <a href="https://github.com/tudelft/sniffy-bug">code</a>
              <p></p>
              <p>We have developed a swarm of autonomous, tiny drones that is able to localize gas sources in unknown, cluttered environments. Bio-inspired AI allows the drones to tackle this complex task without any external infrastructure.</p>
            </td>


            </tr>

          <tr onmouseout="tinyrl_stop()" onmouseover="tinyrl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tinyrl_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/tinyrl.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/tinyrl.jpg' width="160">
              </div>
              <script type="text/javascript">
                function tinyrl_start() {
                  document.getElementById('tinyrl_image').style.opacity = "1";
                }

                function tinyrl_stop() {
                  document.getElementById('tinyrl_image').style.opacity = "1";
                }
                tinyrl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9d61bdd2dc56c22a5d93cb78f07a70ebe3521052.pdf">
              <papertitle>Tiny Robot Learning (tinyRL) for Source Seeking
                on a Nano Quadcopter</papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://scholar.harvard.edu/srivatsan-krishnan">Srivatsan Krishnan</a>,
              Jonathan J. Cruz, 
              <a href="https://www.colbybanbury.com/">Colby R. Banbury</a>,
              William Fu, 
              <a href="https://www.afaust.info/">Aleksandra Faust</a>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido C.H.E. de Croon</a>,
              <a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janapa Reddi</a>
              <br>
							<em>ICRA</em>, 2021 
              <br>
      
              <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9d61bdd2dc56c22a5d93cb78f07a70ebe3521052.pdf">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=wmVKbX7MOnU">video</a>
              /
              <a href="https://github.com/harvard-edge/source-seeking">code</a>
              <p></p>
              <p>We present fully autonomous source seeking onboard a highly constrained nano quadcopter, by contributing application-specific system and observation feature design to enable inference of a deep-RL policy onboard a nano quadcopter. </p>
            </td>


            </tr>

            <tr onmouseout="delfly_stop()" onmouseover="delfly_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='delfly_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/delfly.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/delfly.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function delfly_start() {
                    document.getElementById('delfly_image').style.opacity = "1";
                  }
  
                  function delfly_stop() {
                    document.getElementById('delfly_image').style.opacity = "1";
                  }
                  delfly_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.worldscientific.com/doi/10.1142/S2301385020500235">
                <papertitle>A Tailless Flapping Wing MAV Performing Monocular Visual Servoing Tasks  </papertitle>
                </a>
                <br>
                Diana A. Olejnik, 
                <strong>Bardienus P. Duisterhof</strong>,
                <a href="https://www.matejkarasek.eu/wp/">Matej KarÃ¡sek </a>, 
                Kirk Y. W. Scheper, 
                Tom van Dijk,
                <a href="http://www.bene-guido.eu/wordpress/"> Guido C.H.E. de Croon</a>
                <br>
                <em>Unmanned Systems, Vol. 08, No. 04, pp. 287-294 </em>, 2020 
                <br>
        
                <a href="https://www.worldscientific.com/doi/10.1142/S2301385020500235">paper</a>
                /
                <a href="https://www.youtube.com/watch?v=GoZ1WmADGbk">video</a>
                <p></p>
                <p>This paper describes the computer vision and control algorithms used to achieve autonomous flight with the âˆ¼30g tailless flapping wing robot, used to participate in the International Micro Air Vehicle Conference and Competition (IMAV 2018) indoor microair vehicle competition. </p>
              </td>
  
  
              </tr>



        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href=" https://16820advancedcv.github.io/">16-820 at CMU: Advanced Computer Vision</a>

            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <a href="https://sites.google.com/andrew.cmu.edu/16-720-spring-2024/home">16-720 at CMU: Introduction to Computer Vision</a>

            </td>
            <tr>
              <td width="75%" valign="center">
                <a href="https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=38992">AE2235-I: Aerospace Systems & Control Theory</a>
  
              </td>
            </tr>
          </tr>
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Media Coverage</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href="https://www.forbes.com/sites/davidhambling/2021/07/15/watch-this-autonomous-microdrone-swarm-sniff-out-a-gas-leak/">Forbes</a> <br>
              <a href="https://spectrum.ieee.org/video-friday-fluidic-fingers">IEEE Spectrum Video Friday</a> <br>
              <a href="https://robohub.org/sniffy-bug-a-fully-autonomous-swarm-of-gas-seeking-nano-quadcopters-in-cluttered-environments/">Robohub</a> <br>
              <a href="https://www.bitcraze.io/author/bart/">Bitcraze Blog</a> <br>
              <a href="https://www.youtube.com/watch?v=BJhYaLf_n8g"> PiXL Drone Show</a>

            </td>
          </tr>
          
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href="https://www.tudelft.nl/en/stories/articles/stronger-together-swarm-mini-drones-sniff-out-gas-leaks">Best Graduate in Engineering, TU Delft, academic year 2020-2021</a> <br>
              <a href="https://www.tudelft.nl/en/stories/articles/stronger-together-swarm-mini-drones-sniff-out-gas-leaks">Best Graduate in Aerospace Engineering, TU Delft, academic year 2020-2021</a> <br>
              <a href="https://www.youtube.com/watch?v=GoZ1WmADGbk">Innovation Award, IMAV 2018 Autonomous Drone Race</a> <br>           

            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Modified version of template from <a href="https://github.com/jonbarron/jonbarron_website"> here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
