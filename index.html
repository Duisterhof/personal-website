<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bardienus Pieter Duisterhof</title>
  
  <meta name="author" content="Bardienus Duisterhof">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

  <!-- Global site tag (gtag.js) - Google Analytics DO NOT COPY THIS -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-75286303-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-75286303-1');
</script>

</head>
 
<body>
  <a id="top"></a>
  <div class="bg-orbs"><span class="orb orb-a"></span><span class="orb orb-b"></span></div>
  <header class="navbar">
    <div class="container nav-inner">
      <a href="#top" class="brand">
        <div class="brand-mark"></div>
        <div class="brand-text">
          <div class="brand-sub">Portfolio</div>
          <div class="brand-name">Bardienus Duisterhof</div>
        </div>
      </a>
      <nav class="nav-links">
        <a href="#news">News</a>
        <a href="#research">Research</a>
        <a href="#teaching">Teaching</a>
        <a href="#media">Media</a>
        <a href="#awards">Awards</a>
        <a href="data/CV.pdf" target="_blank" class="btn-outline">CV</a>
      </nav>
    </div>
  </header>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <a id="news"></a>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bardienus Pieter Duisterhof</name>
              </p>
                <p style="text-align:center">
                  <strong>Email:</strong> bduister@cmu.edu
                </p>
             <strong>Bio </strong> </p>
             I am a Ph.D. student at Carnegie Mellon University (CMU), in the <a href="https://www.ri.cmu.edu/"> Robotics Institute</a> advised by <a href="https://ichnow.ski/">Jeffrey Ichnowski</a> and frequently collaborate with <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a> and <a href="https://wenbowen123.github.io/">Bowen Wen</a>.
             I was previously a research intern in the DUSt3R Group at NAVER Labs Europe, advised by <a href="https://europe.naverlabs.com/people_user_naverlabs/j%C3%A9rome-revaud/">JÃ©rome Revaud</a> and <a href="https://europe.naverlabs.com/people_user_naverlabs/vincent-leroy/">Vincent Leroy</a>.
             My interests lie at the intersection of perception and robot manipulation of challenging objects, such as (transparent) deformables. 
             I am a recipient of the 2023 <a href="https://www.cs.cmu.edu/cmlh/">CMLH Fellowship in Digital Health Innovation</a>, and the 2024 <a href="https://www.cs.cmu.edu/cmlh/2024-generative-ai-fellows">CMLH Fellowship in Generative AI in Healthcare</a>. 
             
             
              </p>
             During my first year at CMU I worked with <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/"> Sebastian Scherer </a>, where we focused on geometric camera calibration.        
             Prior to that I completed a Bachelor's and Master's degree in Aerospace Engineering at Delft University of Technology, in the Netherlands. Advised by <a href="http://www.bene-guido.eu/wordpress/">Guido de Croon</a>, I studied efficient bio-inspired algorithms for fully autonomous nano drones.
             In 2019 I was a visiting student at <a href="https://edge.seas.harvard.edu/">Vijay Janapa Reddi's Edge Computing lab</a>, at Harvard University, where we studied Deep Reinforcement Learning for tiny robots.  
            </p>
             I am passionate about creating a future where complex robotic automation is scalable, safe, and beneficial. 
            </p>
            <strong>Students </strong> </p> I am always looking for collaborators, shoot me an email if you would like to work with me!

              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=LLsYMFYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/bduisterhof">Twitter</a> &nbsp/&nbsp
                <a href="https://bsky.app/profile/bardienus.bsky.social">Bluesky</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/duisterhof/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/duisterhof/">Github</a>
              
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <a id="research"></a>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <!--bullet points with news-->
              <ul>
                <li>
                  [September, 2025] <a href="https://rayst3r.github.io/">RaySt3R</a> has been accepted at <a href="https://nips.cc/">NeurIPS 2025</a>! See you in San Diego  ðŸŒ´
                </p>
                </li>
                
                
                <li>
                  [June, 2025] We are excited to release <a href="https://rayst3r.github.io/">RaySt3R</a>, a method for predicting novel depth maps for zero-shot object completion!
                </p>
                </li>
                <li>
                  [March, 2025] MASt3R-SfM has received the best student paper award at <a href="https://3dvconf.github.io/2025/">3DV 2025</a>! Thanks to all collaborators at <a href="https://europe.naverlabs.com/">Naver Labs Europe</a> ðŸ§—
                </p>
                </li>
                <li>
                  [December, 2024] MASt3R-SfM and DynOMo have been accepted at <a href="https://3dvconf.github.io/2025/">3DV 2025</a>!
                </p>
                </li>
                <li>
                  [November, 2024]  Thanks to the <a href="https://www.cs.cmu.edu/cmlh/2024-generative-ai-fellows">CMLH Fellowship in Generative AI in Healthcare</a> for generously supporting my research!</a>
                </p>
                </li>

                <li>
                  [September, 2024] <a href="https://kth-rpl.github.io/cloth-splatting/">Cloth-Splatting</a> has been accepted at <a href="https://www.corl.org/">CoRL 2024</a>!
                </p>
                </li>
                <li>
                  [August, 2024] <a href="https://deformgs.github.io/">DeformGS</a> has been accepted at <a href="https://www.algorithmic-robotics.org/">WAFR 2024</a>!
                </p>
                </li>
                <li>
                  [July, 2024] I started my internship at NAVER Labs Europe! Excited to work with JÃ©rome Revaud, Vincent Leroy and the rest of the DUSt3R team. 
                </p>
                </li>
                
                <li>                [January, 2024] 2 papers accepted at <a href="https://2024.ieee-icra.org/">ICRA 2024</a>! See you in Japan ðŸ‡¯ðŸ‡µ. </p>

                </li>
                <li>                [November, 2023] Check out our recent work on  <a href="https://md-splatting.github.io">MD-Splatting</a>, a method for dense tracking and novel view synthesis of cloth ðŸ§£. </p>
                </li>
                <li>                [July, 2023] Our paper on NeRFs for transparent objects has been accepted for a ðŸŒŸspotlightðŸŒŸ presentation at the  <a href="https://sites.google.com/view/iccv23trickyworkshop">ICCV23 - TRICKY Workshop</a>. </p>
                </li>
                <li>                [April, 2023] Thanks to the <a href="https://www.cs.cmu.edu/cmlh/">CMLH Fellowship in Digital Health Innovation</a> for generously supporting my research! </p>
                </li>
              </ul>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="rayst3r_stop()" onmouseover="rayst3r_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rayst3r-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/rayst3r.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
              <script type="text/javascript">
                function rayst3r_start() {
                  document.getElementById('rayst3r-vid').style.opacity = "1";
                }

                function rayst3r_stop() {
                  document.getElementById('rayst3r-vid').style.opacity = "1";
                }
                rayst3r_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://rayst3r.github.io">
              <papertitle>RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://www.ri.cmu.edu/ri-people/jan-oberst/">Jan Oberst</a>,
              <a href="https://wenbowen123.github.io/">Bowen Wen</a>
              <br>
              <a href="https://research.nvidia.com/person/stan-birchfield">Stan Birchfield</a>,
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>
              <br>
							<em> NeurIPS 2025</em> 
              <br>
              <a href="https://rayst3r.github.io/">project website</a>
              /
              <a href="https://github.com/Duisterhof/rayst3r">code</a>
              /
              <a href="https://x.com/BDuisterhof/status/1930986607655649647">X thread</a>
              <p></p>
              <p>Imagine if robots could fill in the blanks in cluttered scenes.

                 Enter <span style="color: orange;">RaySt3R</span>âœ¨: a single masked RGB-D image in, complete 3D out.
                It infers depth, object masks, and confidence for novel views, and merges the predictions into a single point cloud. </p>
            </td>
          </tr>


          <tr onmouseout="mast3r_sfm_stop()" onmouseover="mast3r_sfm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deformgs-vid'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
              <img src="images/mast3r.png" width="160">
              
              </div>
              <script type="text/javascript">
                function mast3r_sfm_start() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }

                function mast3r_sfm_stop() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }
                mast3r_sfm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://europe.naverlabs.com/research/publications/mast3r-sfm-a-fully-integrated-solution-for-unconstrained-structure-from-motion/">
              <papertitle>MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof*</strong>,
              <a href="https://www.vicos.si/people/lojze_zust/">Lojze Zust*</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/pweinzae/">Philippe Weinzaepfel</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/vleroy/">Vincent Leroy</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/ycabon/">Yohann Cabon</a>,
              <a href="https://europe.naverlabs.com/people_user_naverlabs/jrevaud">JÃ©rome Revaud</a>
              <br>
							<em>International Conference on 3D Vision (3DV) 2025, <span style="color: orange;">Oral, Best Student Paper Award</span></em> 
              <br>
              <a href="https://arxiv.org/abs/2409.19152">arxiv</a>
              /
              <a href="https://github.com/naver/mast3r/tree/mast3r_sfm">code</a>
              <p></p>
              <p>MASt3R for SfM with 1000+ unordered images! We contribute a memory-efficient algorithm which leverages the MASt3R encoder for image retrieval without any overhead. 
                MASt3R-SfM has an overall linear complexity in the number of images, and can handle any set of ordered or unordered images.   </p>
            </td>
          </tr>



          <tr onmouseout="deformgs_stop()" onmouseover="deformgs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deformgs-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/deformgs.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
              <script type="text/javascript">
                function deformgs_start() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }

                function deformgs_stop() {
                  document.getElementById('deformgs-vid').style.opacity = "1";
                }
                deformgs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://deformgs.github.io">
              <papertitle>DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://mandizhao.github.io/">Zhao Mandi</a>,
              <a href="https://www.ri.cmu.edu/ri-people/yunchao-yao/">Yunchao Yao</a>,
              <a href="https://jia-wei-liu.github.io/">Jia-Wei Liu</a>,
              <a href="https://jennyseidenschwarz.github.io/">Jenny Seidenschwarz</a>,
              <a href="https://sites.google.com/view/showlab">Mike Zheng Shou</a>,
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
              <a href="https://shurans.github.io/">Shuran Song</a>,
              <a href="https://research.nvidia.com/person/stan-birchfield">Stan Birchfield</a>,
              <a href="https://wenbowen123.github.io/">Bowen Wen</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>
              <br>
							<em> Proc. Algorithmic Foundations of Robotics (WAFR) 2024</em> 
              <br>
              <a href="https://md-splatting.github.io/">project website</a>
              /
              <a href="https://arxiv.org/abs/2312.00583">arXiv</a>
              /
              <a href="https://drive.google.com/drive/folders/116XTLBUvuiEQPjKXKZP8fYab3F3L1cCd?usp=sharing">data</a>
              /
              <a href="https://github.com/momentum-robotics-lab/md-splatting">code</a>
              /
              <a href="https://x.com/BDuisterhof/status/1829170197829431695">X thread</a>
              <p></p>
              <p>Deformable objects are common in household, industrial and healthcare settings. Tracking them would unlock many applications in robotics, gen-AI, and AR.

                How? Check out DeformGS: a method for dense 3D tracking and dynamic novel view synthesis on deformable cloths in the real world.</p>
            </td>
          </tr>




          <tr onmouseout="clothsplatting_stop()" onmouseover="clothsplatting_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='clothsplatting-vid'><video  width=100% height=100% muted autoplay loop>
                <source src="images/clothsplatting.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video></div>
              </div>
              
              
              <script type="text/javascript">
                function clothsplatting_start() {
                  document.getElementById('clothsplatting-vid').style.opacity = "1";
                }

                function clothsplatting_stop() {
                  document.getElementById('clothsplatting-vid').style.opacity = "1";
                }
                clothsplatting_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://kth-rpl.github.io/cloth-splatting/">
              <papertitle>Cloth-Splatting: 3D State Estimation from RGB Supervision for Deformable Objects
              </papertitle>
              </a>
              <br>
              <a href="https://albilo17.github.io/">Alberta Longhini*</a>,
              <a href="https://buesma.github.io/">Marcel BÃ¼sching*</a>,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://jenslundell.ai/">Jens Lundell</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="https://www.kth.se/profile/celle">MÃ¥rten BjÃ¶rkman</a>
              <a href="https://www.kth.se/profile/dani">Danica Kragic</a>

              <br>
							<em>Conference on Robot Learning (CoRL) 2024</em> 
              <br>
              <a href="https://kth-rpl.github.io/cloth-splatting/">project website</a>
              /
              <a href="https://openreview.net/forum?id=WmWbswjTsi">paper</a>
              <p></p>
              <p> We present Cloth-Splatting: a method for accurate state estimation of deformable objects from RGB supervision. 
                Cloth-Splatting leverages a GNN as a prior to improve tracking accuracy and speed up convergence.</p>
            </td>
          </tr>


          <tr onmouseout="dynomo_stop()" onmouseover="dynomo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dynomo-vid'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                <source src='images/dynomo.mp4' width="160" type="video/mp4">
              
                </video></div>
              </div>
              
              
              <script type="text/javascript">
                function dynomo_start() {
                  document.getElementById('dynomo-vid').style.opacity = "1";
                }

                function dynomo_stop() {
                  document.getElementById('dynomo-vid').style.opacity = "1";
                }
                dynomo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.02104">
              <papertitle>DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction
              </papertitle>
              </a>
              <br>
              <a href="https://jennyseidenschwarz.github.io/">Jenny Seidenschwarz</a>,
              <a href="https://dvl.in.tum.de/team/zhouq/">Qunjie Zhou</a>,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
              <a href="https://dvl.in.tum.de/team/lealtaixe/">Laura Leal-TaixÃ©</a>

              <br>
							<em>International Conference on 3D Vision (3DV) 2025</em> 
              <br>
              <a href="https://arxiv.org/abs/2409.02104">arXiv /</a>
              <a href="https://jennyseidenschwarz.github.io/DynOMo.github.io/">project website</a>

              <p></p>
              <p> Online 3D tracking can unlock many new applications in robotics, AR and VR. Most prior works have focused on offline tracking, requiring an entire sequence of posed images. 
                Here we present DynOMo, a method for simultaneous 3D tracking, 3D reconstruction, novel view synthesis and pose estimation!</p>
            </td>
          </tr>

          <tr onmouseout="residualnerf_stop()" onmouseover="residualnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='residualnerf_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/residual_nerf.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function residualnerf_start() {
                  document.getElementById('residualnerf_image').style.opacity = "1";
                }

                function residualnerf_stop() {
                  document.getElementById('residualnerf_image').style.opacity = "1";
                }
                residualnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://residual-nerf.github.io/">
              <papertitle>Residual-NeRF: Learning Residual NeRFs for Transparent Object Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://yueminm.github.io/">Yuemin Mao</a>,
              <a href="https://www.ri.cmu.edu/ri-people/si-heng-teng/">Si Heng Teng</a>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>
              <br>
							<em>IEEE International Conference on Robotics and Automation (ICRA) 2024</em> <br> 
              <em>ðŸŒŸSpotlightðŸŒŸ presentation at the  <a href="https://sites.google.com/view/iccv23trickyworkshop">ICCV23 - TRICKY Workshop</em>
              <br>
              <a href="https://residual-nerf.github.io/">project website</a>
              /
              <a href="https://arxiv.org/abs/2405.06181">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=oA2CkRFI61c">video</a>
              /
              <a href="https://github.com/momentum-robotics-lab/residual-nerf">code</a>
              <p></p>
              <p>In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. 
                By first learning a background NeRF of the scene without transparent objects to be manipulated, we improve depth perception quality and speed up training. 
              </p>
            </td>
          </tr>

          <tr onmouseout="gslbench_stop()" onmouseover="gslbench_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gslbench_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/gslbench.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function gslbench_start() {
                  document.getElementById('gslbench_image').style.opacity = "1";
                }

                function gslbench_stop() {
                  document.getElementById('gslbench_image').style.opacity = "1";
                }
                gslbench_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/gslbench/home">
              <papertitle>GSL-Bench: High Fidelity Gas Source Localisation Benchmarking
              </papertitle>
              </a>
              <br>
              Hajo H. Erwich,
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido de Croon</a>
              <br>
							<em>IEEE International Conference on Robotics and Automation (ICRA) 2024</em> 
              <br>
              <a href="https://sites.google.com/view/gslbench/home">project website</a>
              /
              <a href="https://ieeexplore.ieee.org/document/10610755">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=kZa48WXf_1w">video</a>
              /
              <a href="https://github.com/Herwich012/GSL-Bench">code</a>
              <p></p>
              <p>Gas-source localization is an important task for autonomous robots. We present GSL-Bench, the first standardized benchmark for gas-source localization. 
              GSL-Bench uses NVIDIA Isaac Sim for high visual fidelity, and OpenFOAM for realistic gas simulations. 
              </p>
            </td>
          </tr>



          <tr onmouseout="tartancalib_stop()" onmouseover="tartancalib_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tartan_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                <source src="images/tartancalib.mp4" type="video/mp4"> 
                </video></div>
                
              </div>
              <script type="text/javascript">
                function tartancalib_start() {
                  document.getElementById('tartan_image').style.opacity = "1";
                }

                function tartancalib_stop() {
                  document.getElementById('tartan_image').style.opacity = "1";
                }
                tartancalib_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://tartancalib.com">
              <papertitle>TartanCalib: Iterative Wide-Angle Lens Calibration using Adaptive SubPixel Refinement of AprilTags
              </papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="http://www.huyaoyu.com/">Yaoyu Hu</a>,
              <a href="https://www.ri.cmu.edu/ri-people/si-heng-teng/">Si Heng Teng</a>,
              <a href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/">Sebastian Scherer</a>
              <a href="https://tartancalib.com">project website</a>
              /
              <a href="https://arxiv.org/abs/2210.02511">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=8WCCS-Ag_Co">video</a>
              /
              <a href="https://github.com/castacks/tartancalib">code</a>
              <p></p>
              <p>In this work we present our methodology for accurate wide-angle calibration. Our pipeline generates an intermediate model, and leverages it to iteratively improve feature detection and eventually the camera parameters. </p>
            </td>
          </tr>

          <tr onmouseout="sniffy_stop()" onmouseover="sniffy_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sniffy_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/sniffy_square.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/sniffy.png' width="160">
              </div>
              <script type="text/javascript">
                function sniffy_start() {
                  document.getElementById('sniffy_image').style.opacity = "1";
                }

                function sniffy_stop() {
                  document.getElementById('sniffy_image').style.opacity = "1";
                }
                sniffy_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2107.05490">
              <papertitle>Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments</papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://shushuai3.github.io/">Shushuai Li</a>,
              <a href="https://www.javierburgues.com/">Javier BurguÃ©s</a>,
              <a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janapa Reddi</a>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido C.H.E. de Croon</a>
              <br>
							<em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2021</em>
              <br>
      
              <a href="https://arxiv.org/abs/2107.05490">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=hj_SBSpK5qg">video</a>
              /
              <a href="https://github.com/tudelft/sniffy-bug">code</a>
              <p></p>
              <p>We have developed a swarm of autonomous, tiny drones that is able to localize gas sources in unknown, cluttered environments. Bio-inspired AI allows the drones to tackle this complex task without any external infrastructure.</p>
            </td>


            </tr>

          <tr onmouseout="tinyrl_stop()" onmouseover="tinyrl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tinyrl_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/tinyrl.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/tinyrl.jpg' width="160">
              </div>
              <script type="text/javascript">
                function tinyrl_start() {
                  document.getElementById('tinyrl_image').style.opacity = "1";
                }

                function tinyrl_stop() {
                  document.getElementById('tinyrl_image').style.opacity = "1";
                }
                tinyrl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9d61bdd2dc56c22a5d93cb78f07a70ebe3521052.pdf">
              <papertitle>Tiny Robot Learning (tinyRL) for Source Seeking
                on a Nano Quadcopter</papertitle>
              </a>
              <br>
              <strong>Bardienus P. Duisterhof</strong>,
              <a href="https://scholar.harvard.edu/srivatsan-krishnan">Srivatsan Krishnan</a>,
              Jonathan J. Cruz, 
              <a href="https://www.colbybanbury.com/">Colby R. Banbury</a>,
              William Fu, 
              <a href="https://www.afaust.info/">Aleksandra Faust</a>,
              <a href="http://www.bene-guido.eu/wordpress/">Guido C.H.E. de Croon</a>,
              <a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janapa Reddi</a>
              <br>
							<em>IEEE International Conference on Robotics and Automation (ICRA) 2021</em> 
              <br>
      
              <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9d61bdd2dc56c22a5d93cb78f07a70ebe3521052.pdf">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=wmVKbX7MOnU">video</a>
              /
              <a href="https://github.com/harvard-edge/source-seeking">code</a>
              <p></p>
              <p>We present fully autonomous source seeking onboard a highly constrained nano quadcopter, by contributing application-specific system and observation feature design to enable inference of a deep-RL policy onboard a nano quadcopter. </p>
            </td>


            </tr>

            <tr onmouseout="delfly_stop()" onmouseover="delfly_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='delfly_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/delfly.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/delfly.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function delfly_start() {
                    document.getElementById('delfly_image').style.opacity = "1";
                  }
  
                  function delfly_stop() {
                    document.getElementById('delfly_image').style.opacity = "1";
                  }
                  delfly_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.worldscientific.com/doi/10.1142/S2301385020500235">
                <papertitle>A Tailless Flapping Wing MAV Performing Monocular Visual Servoing Tasks  </papertitle>
                </a>
                <br>
                Diana A. Olejnik, 
                <strong>Bardienus P. Duisterhof</strong>,
                <a href="https://www.matejkarasek.eu/wp/">Matej KarÃ¡sek </a>, 
                Kirk Y. W. Scheper, 
                Tom van Dijk,
                <a href="http://www.bene-guido.eu/wordpress/"> Guido C.H.E. de Croon</a>
                <br>
                <em>Unmanned Systems, Vol. 08, No. 04, pp. 287-294 </em>, 2020 
                <br>
        
                <a href="https://www.worldscientific.com/doi/10.1142/S2301385020500235">paper</a>
                /
                <a href="https://www.youtube.com/watch?v=GoZ1WmADGbk">video</a>
                <p></p>
                <p>This paper describes the computer vision and control algorithms used to achieve autonomous flight with the âˆ¼30g tailless flapping wing robot, used to participate in the International Micro Air Vehicle Conference and Competition (IMAV 2018) indoor microair vehicle competition. </p>
              </td>
  
  
              </tr>



        </tbody></table>

        <a id="teaching"></a>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href=" https://16820advancedcv.github.io/">16-820 at CMU: Advanced Computer Vision</a>

            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <a href="https://sites.google.com/andrew.cmu.edu/16-720-spring-2024/home">16-720 at CMU: Introduction to Computer Vision</a>

            </td>
            <tr>
              <td width="75%" valign="center">
                <a href="https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=38992">AE2235-I: Aerospace Systems & Control Theory</a>
  
              </td>
            </tr>
          </tr>
          
        </tbody></table>

        <a id="media"></a>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Media Coverage</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href="https://www.forbes.com/sites/davidhambling/2021/07/15/watch-this-autonomous-microdrone-swarm-sniff-out-a-gas-leak/">Forbes</a> <br>
              <a href="https://spectrum.ieee.org/video-friday-fluidic-fingers">IEEE Spectrum Video Friday</a> <br>
              <a href="https://robohub.org/sniffy-bug-a-fully-autonomous-swarm-of-gas-seeking-nano-quadcopters-in-cluttered-environments/">Robohub</a> <br>
              <a href="https://www.bitcraze.io/author/bart/">Bitcraze Blog</a> <br>
              <a href="https://www.youtube.com/watch?v=BJhYaLf_n8g"> PiXL Drone Show</a>

            </td>
          </tr>
          
        </tbody></table>
        <a id="awards"></a>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a href="https://europe.naverlabs.com/research/publications/mast3r-sfm-a-fully-integrated-solution-for-unconstrained-structure-from-motion/">MASt3R-SfM Best Student Paper Award, 3DV 2025</a> <br>
              <a href="https://www.tudelft.nl/en/stories/articles/stronger-together-swarm-mini-drones-sniff-out-gas-leaks">Best Graduate in Engineering, TU Delft, academic year 2020-2021</a> <br>
              <a href="https://www.tudelft.nl/en/stories/articles/stronger-together-swarm-mini-drones-sniff-out-gas-leaks">Best Graduate in Aerospace Engineering, TU Delft, academic year 2020-2021</a> <br>
              <a href="https://www.youtube.com/watch?v=GoZ1WmADGbk">Innovation Award, IMAV 2018 Autonomous Drone Race</a> <br>           

            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Modified version of template from <a href="https://github.com/jonbarron/jonbarron_website"> here</a>, and vibe-coded with Cursor.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
